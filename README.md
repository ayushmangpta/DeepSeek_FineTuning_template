# ğŸš€ DeepSeek Model Fine-tuning Pipeline

A comprehensive Jupyter notebook for fine-tuning DeepSeek language models using LoRA (Low-Rank Adaptation) on custom datasets. This pipeline provides an efficient and memory-optimized approach to train custom language models.

## ğŸŒŸ Features

- Complete environment setup and dependency management guide
- Optimized model configuration with LoRA
- Automated data preparation and processing
- Configurable training pipeline
- Model saving and export utilities
- Memory-efficient training with 4-bit quantization
- Comprehensive troubleshooting guide

## ğŸ› ï¸ Prerequisites

- Python 3.8+
- CUDA toolkit (11.8 recommended)
- NVIDIA GPU with sufficient VRAM

## ğŸš€ Usage

1. Open the Jupyter notebook:
```bash
jupyter notebook FineTuning_DeepSeek.ipynb
```

2. Configure your dataset path in the data preparation section:
```python
DATA_FILE = 'path/to/your/dataset.csv'
```

3. Adjust training parameters as needed:
- Model selection (`MODEL_NAME`)
- Sequence length (`MAX_SEQ_LENGTH`)
- Batch size and gradient accumulation
- Learning rate and number of epochs

4. Run all cells in sequence

## ğŸ“Š Training Configuration

The notebook includes several configurable parameters:

- `MAX_SEQ_LENGTH`: Maximum sequence length (default: 1024)
- `per_device_train_batch_size`: Batch size per GPU (default: 1)
- `gradient_accumulation_steps`: Steps before gradient update (default: 8)
- `learning_rate`: Training learning rate (default: 2e-5)
- `num_train_epochs`: Number of training epochs (default: 3)

## ğŸ”§ LoRA Configuration

Default LoRA settings:
```python
r=16                    # Rank dimension
lora_alpha=32          # LoRA scaling factor
lora_dropout=0.1       # Dropout probability
target_modules=[       # Layers to fine-tune
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj"
]
```

## ğŸ’¾ Output

The fine-tuned model and tokenizer are saved in the `fine_tuned_model` directory by default. The directory structure will be:

```
fine_tuned_model/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.bin
â””â”€â”€ tokenizer_config.json
```

## âš ï¸ Troubleshooting

### Common Issues

1. CUDA Out of Memory:
   - Reduce batch size
   - Decrease sequence length
   - Increase gradient accumulation steps
   - Enable gradient checkpointing

2. Slow Training:
   - Increase batch size (if memory allows)
   - Adjust learning rate
   - Consider using a smaller model variant

### Best Practices

- Monitor GPU memory usage
- Start with small datasets for testing
- Use gradient accumulation for larger effective batch sizes
- Save checkpoints regularly

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“« Contact

- ayushmangpta@gmail.com
- workwithshivansh22@gmail.com
